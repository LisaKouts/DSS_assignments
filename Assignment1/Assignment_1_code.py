# -*- coding: utf-8 -*-
"""Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zBIu3srDyoIkZw-D_c7p74iZXapn_4Jc

# Machine Learning Challenge Group 28

## Task 1
"""
import cv2
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from keras.models import Sequential, load_model
from keras.layers import BatchNormalization,MaxPooling2D, MaxPooling1
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D
from keras.optimizers import Adam
from tabulate import tabulate
from sklearn import datasets, svm, metrics
from keras.utils import to_categorical, normalize
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelBinarizer

with np.load('training-dataset.npz') as data:
    img = data['x']
    ibl = data['y']

type(img)

##SPLITTING DATA 

# Inputs
X = np.array(img, dtype='float32')
# Output
y = np.array(ibl, dtype='float32')

#splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=999)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/3, random_state=999)

#26 classes from 1~26 to 0~25
y_test = y_test-1
y_train = y_train - 1 
y_val = y_val-1

print(np.unique(y_val))

"""## MODEL BUILDING: ATTEMPT 1"""

##NORMALIZATION

X_train_base = normalize(X_train, axis = 1 )
X_val_base = normalize(X_val, axis = 1 )
X_test_base = normalize(X_test, axis = 1 )

## BINANARIZE TARGET

onehot = LabelBinarizer()
y_train_base = onehot.fit_transform(y_train)
y_val_base = onehot.transform(y_val)
y_test_base = onehot.transform(y_test)

model_base = tf.keras.models.Sequential() 
model_base.add(tf.keras.layers.Flatten()) 
model_base.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) 
model_base.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))
model_base.add(tf.keras.layers.Dense(26, activation = tf.nn.softmax))
model_base.compile(optimizer = "adam",
             loss = "categorical_crossentropy",
             metrics = ["accuracy"])
model_base.fit(X_train_base, y_train_base, validation_data=(X_val_base, y_val_base), epochs = 10)

## Accuracy is not high enough, therefore this model will not be used

val_loss, val_accuracy = model_base.evaluate(X_test_base, y_test_base)
print(val_loss, val_accuracy)

"""## PREPROCESSING FOR CNN"""

## VISUALIZATION

images_and_labels = list(zip(X_test.reshape((X_test.shape[0],28,28)), y_test))

for index, (image, label) in enumerate(images_and_labels[:5]):
    plt.subplot(1, 5, index + 1)
    plt.axis('off')
    plt.imshow(image,cmap=plt.cm.gray_r)
    plt.title('%i' % label)

image.shape

##NORMALIZATION -- checking if accuracy improves

X_train_norm = normalize(X_train, axis = 1 )
X_val_norm = normalize(X_val, axis = 1 )
X_test_norm = normalize(X_test, axis = 1 )

## RESHAPING DATA 

X_train_main = X_train_norm.reshape(66560, 28, 28, 1)
X_val_main = X_val_norm.reshape(33280, 28, 28, 1)
X_test_main = X_test_norm.reshape(24960, 28, 28, 1)

## ENCODING
y_train_main = to_categorical(y_train) 
y_val_main = to_categorical(y_val) 
y_test_main = to_categorical(y_test)

"""## MODEL BUILDING: ATTEMPT 2"""

##MODEL BUILDING

## Declare the model
model = Sequential()

## Add the layers to the model
model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dropout(.5))
model.add(Dense(26, activation='softmax'))

model.summary()

## Accuracy of this model in the test set was 89.5%, therefore it will not be used either.

"""## MAIN MODEL: ATTEMPT 3"""

model = Sequential()

model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size = 3, activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))

model.add(Conv2D(64, kernel_size = 3, activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size = 3, activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.4))

model.add(Conv2D(128, kernel_size = 4, activation='relu'))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dropout(0.4))
model.add(Dense(26, activation='softmax'))


model.compile(loss= "categorical_crossentropy",
              optimizer='adam',
              metrics=['accuracy'])

basic_history_3 = model.fit(X_train_main, y_train_main, validation_data=(X_val_main, y_val_main), epochs = 10)

test_loss, test_accuracy = model.evaluate(X_test_main, y_test_main)
print(test_loss, test_accuracy)

## LOSS AND ACCURACY VIZUALIZATION PER EPOCH

history = basic_history_3

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.margins(0,1)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0.7,1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.margins(0,1)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0,0.8)
plt.show()

"""## EVALUATION OF MAIN MODEL"""

#MULTICLASS CONFUSION MATRIX 

#prediction values into a list

pred = []
for i in range(y_test_main.shape[0]):
    pred.append(np.argmax(hard_maxed_prediction[i]))
print(pred[:10])

#prediction values into a list 
true = []
for i in range(y_test_main.shape[0]):
    true.append(np.argmax(y_test_main[i]))
print(true[:10])

def multiclass_confusion_matrix(y_true, y_pred): 
    N = len(np.unique(y_true))
    M = np.zeros((N, N))
    for true, pred in zip(y_true, y_pred):
        M[y_pred, y_true] += 1
    return M
    
print(multiclass_confusion_matrix(true, pred))

#OTHER EVALUATION METRICS: CLASS SCORES 
from tabulate import tabulate

def class_scores(y_true, y_pred, reference):
    Y_true = set([ i for (i,v) in enumerate(y_true) if v == reference])
    Y_pred = set([ i for (i,v) in enumerate(y_pred) if v == reference])
    TP = len(Y_true.intersection(Y_pred))
    FP = len(Y_pred - Y_true)
    FN = len(Y_true - Y_pred)
    return TP, FP, FN

score_list = []

for t in range(26): 
    (TP, FP, FN) = class_scores(true, pred, t)
    p = TP / (TP + FP)
    r = TP / (TP + FN)
    f = (p * r) / (p + r)
    score_list.append([t,p,r,f])

class_score_table = tabulate(score_list, headers=["Class", "Precision", "Recall","F-score"])

print(class_score_table)

"""## TASK 2

## PREPROCCESING
"""

## ADDING NOISE 

from skimage.util import random_noise
 
# Add salt-and-pepper noise to the image.
noise_img = random_noise(img, mode='s&p',amount=0.4)
noise_img = np.array(img) + np.array(255*noise_img, dtype = 'uint8')

## VISUAL CHECK 

plt.imshow(img[8].reshape(28, 28), cmap=plt.cm.gray_r)
plt.imshow(noise_img[8].reshape(28, 28), cmap=plt.cm.gray_r)

print(noise_img.shape)
print(img.shape)

## FITTING AGAINST NOISE 

## SPLITTING THE DATA SET 

X2 = np.array(noise_img, dtype='float32')
y = np.array(ibl, dtype='float32')

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size=0.2, random_state=999)
X_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=1/3, random_state=999)

#26 classes from 1~26 to 0~25
y_test2 = y_test2-1
y_train2 = y_train2 - 1 
y_val2 = y_val2-1

## RESHAPING THE DATA 

X_train2 = X_train2.reshape(66560, 28, 28, 1)
X_val2 = X_val2.reshape(33280, 28, 28, 1)
X_test2 = X_test2.reshape(24960, 28, 28, 1)

## ENCODING
y_train2 = to_categorical(y_train2) 
y_val2 = to_categorical(y_val2) 
y_test2 = to_categorical(y_test2)

## COMPILING AND FITTING 

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
noisy_model = model.fit(X_train2, y_train2, validation_data=(X_val2, y_val2), epochs=10)

## LOSS AND ACCURACY VIZUALIZATION PER EPOCH

history = noisy_model

print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.margins(0,1)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0.85,1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.margins(0,1)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0,0.3)
plt.show()

## CHECKING TEST ACCURACY 

test_loss, test_accuracy = model.evaluate(X_test2, y_test2)
print(test_loss, test_accuracy)

## VISUAL CHECK

print ("\n\n--------- Prediction --------- \n\n")
plt.imshow(X_test2[15].reshape(28, 28), cmap=plt.cm.gray_r)
plt.show()
print("\n\nFinal Output: {}".format(np.argmax(hard_maxed_prediction[15])))
print("True Value: {}".format(np.argmax(y_test2[15])))

"""### TEST DATA"""

## IMPORTING AND INSPECTING TEST IMAGES 

import matplotlib.pyplot as plt
test_img = np.load('test-dataset.npy')
test_img.shape 
print(test_img.shape) 
print(test_img[0].shape)

#image size 30x168, 10000 images 
plt.figure()
plt.imshow(test_img[55], cmap=plt.cm.gray_r)

print(test_img.shape)

#28, 54, 84, 112, 140, 168

## Reshaping the test images from 30x168 into 28x168

## Deleting the first and last row from each picture
test_set = []

for i in range(0,test_img.shape[0]):
  one = np.delete(test_img[i],(0),axis=0)
  one = np.delete(one,(-1),axis=0)
  test_set.append(one)

"""### FUNCTIONS"""

## FUNCTION TO MEASURE THE DISTANCE OF OUR PREDICTION FROM THE ACTUAL TARGET

def hamming(a, b):
    distance = 0
    for index in range(min(len(a), len(b))):
        if not a[index] == b[index]:
            distance += 1
    distance += abs(len(a) - len(b))
    return distance

## SLIDING WINDOW FUNCTION

def strided_axis0(a): 
    s0,s1 = a.strides 
    return np.lib.stride_tricks.as_strided(a, shape=(140,28,28), strides=(s1,s0,s1))

## FUNCTION TO PREDICT LABELS OF THE TEST IMAGES

from collections import Counter
from heapq import nlargest

def top_five_accu(image,step,start):
    windows = strided_axis0(image) #140, 28, 28

    all_preds_probs = {}

    for window in range(start,windows.shape[0],step): 
        pred_list = []
        prediction = model.predict(windows[window].reshape(1, 28, 28, 1)) 
        hard_maxed_prediction = np.zeros(prediction.shape) 
        for v in range(prediction.shape[0]):
            hard_maxed_prediction[v][np.argmax(prediction[v])] = 1
        highest_pred = prediction[0][np.argmax(prediction)] 
        all_preds_probs[highest_pred] = np.argmax(hard_maxed_prediction)+1

    #fetching the largest 5 probabilities 
    probs_list = all_preds_probs.keys()
    n = 5   
    c = Counter(nlargest(n, probs_list))
    result = []
    for elem in probs_list:
        if c.get(elem, 0):
            result.append(elem)
            c[elem] -= 1
    answer = []
    for prob in result:
        answer.append(all_preds_probs.get(prob))
    return answer

## GATHERING UP PREDICTIONS AND SORTING BY COUNTS 
import random

def get_predictions(test_image):
    winners = []
    compare = 0
    for step in range(5,20):
        for start in range(0,10,2):
            top_5 = top_five_accu(test_image,step,start)
            winners.append(top_5)

    concatenate = []
    for seq in winners:
        sequence = ""
        for num in seq:
            if num < 10:
                sequence += "0" + str(num)
            else:
                sequence += str(num)
        concatenate.append(sequence)
  
    dic = {}
    for i in range(len(concatenate)):
        dic[concatenate[i]] = concatenate.count(concatenate[i])
    dic = sorted(dic, key=dic.get, reverse=True)
    return dic[:5]

## USE HAMMING DISTANCE TO COMPARE PREDICTIONS TO TARGET
def test_accuracy(correct,top_5_accuracy):
    num = []
    for seq in top_5_accuracy:
        ham = hamming(seq,correct)
        num.append([seq,ham])

    winner = sorted(num, key = lambda x: x [1])
    return [winner[0][0], correct]

"""## PREDICTION"""

## PREDICTION BASED ON TEST SET
results = []

for ind,label in zip(indexes,true_labels):
  predictions = get_predictions(test_set[ind])
  winner,correct=test_accuracy(label,predictions)
  results.append([ind,predictions,correct,winner])

class_score_table = tabulate(results, headers=["Image Index", "Top 5 predictions", "Y true","Y predicted"])
print(class_score_table)

## COMPILING THE CSV FILE

import csv

with open("prediction.csv", "w", newline='') as f:
  writer = csv.writer(f,delimiter=',')
  writer.writerow(['Pred 1','Pred 2','Pred 3','Pred 4','Pred 5'])
  for image in range(0,test_images.shape[0]):
    predictions = get_predictions(test_images[image])
    writer.writerow(tuple(predictions))
